# -*- coding: utf-8 -*-
"""shell ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_NOJJYSp_rAT-ZQLwk06Pssa0ZIU5Eml
"""

import pandas as pd

train = pd.read_csv('train.csv')
test  = pd.read_csv('test.csv')

print("Train shape:", train.shape)  # Expect (2000, 65)
print("Test shape: ", test.shape)   # Expect (500, 56)

print(train.head())

print(train.info())

print(train.describe())

"""step 1 finished

"""

import pandas as pd

# Load your dataset (if not already loaded)
train = pd.read_csv('train.csv')

# Check if component fractions sum to ~1.0
fraction_cols = [f'Component{i}_fraction' for i in range(1, 6)]
train['fraction_sum'] = train[fraction_cols].sum(axis=1)

# See statistics
print(train['fraction_sum'].describe())
print(train['fraction_sum'].value_counts().head())

# Identify target columns
target_cols = [col for col in train.columns if col.startswith('BlendProperty')]

# Identify component properties
property_cols = [col for col in train.columns if 'Property' in col and col not in target_cols]

# Check for missing data
print(train.isnull().sum().sort_values(ascending=False))

# Summary of features
print(train.describe())

# Boxplot of target distributions
plt.figure(figsize=(14, 6))
sns.boxplot(data=train[target_cols])
plt.xticks(rotation=45)
plt.title('Boxplot of Blend Properties')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

corr = train[target_cols].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation between Blend Properties")
plt.show()

# Boxplot of target distributions
plt.figure(figsize=(14, 6))
sns.boxplot(data=train[target_cols])
plt.xticks(rotation=45)
plt.title('Boxplot of Blend Properties')
plt.show()

# Any constant or near-constant columns?
nunique = train.nunique()
print(nunique[nunique <= 1])

"""step 2 finished"""

# Optional: Drop Component5_fraction to reduce multicollinearity
# Or dynamically compute it later if needed
train = train.drop(columns=['Component5_fraction'])
test = test.drop(columns=['Component5_fraction'])

# Recompute Component5_fraction (if dropped)
train['Component5_fraction'] = 1.0 - train[[f'Component{i}_fraction' for i in range(1, 5)]].sum(axis=1)
test['Component5_fraction'] = 1.0 - test[[f'Component{i}_fraction' for i in range(1, 5)]].sum(axis=1)

# Generate weighted average features
for k in range(1, 11):
    train[f'WeightedAvg_Property{k}'] = 0
    test[f'WeightedAvg_Property{k}'] = 0
    for i in range(1, 6):
        train[f'WeightedAvg_Property{k}'] += train[f'Component{i}_fraction'] * train[f'Component{i}_Property{k}']
        test[f'WeightedAvg_Property{k}'] += test[f'Component{i}_fraction'] * test[f'Component{i}_Property{k}']

# 1. Check column presence
expected_weighted_cols = [f'WeightedAvg_Property{i}' for i in range(1, 11)]
expected_target_cols = [f'BlendProperty{i}' for i in range(1, 11)]

print("Weighted Avg Features OK:", all(col in train.columns for col in expected_weighted_cols))
print("Target Columns OK:", all(col in train.columns for col in expected_target_cols))

# 2. Check for NaNs
print("Any NaNs?", train.isnull().sum().sum() > 0)

from sklearn.dummy import DummyRegressor
from sklearn.linear_model import LinearRegression
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error
import pandas as pd

# Assuming you've already loaded and prepared `train` with:
# - Component fractions
# - Component properties
# - WeightedAvg_Property1 to WeightedAvg_Property10
# - BlendProperty1 to BlendProperty10

# Define features and targets
weighted_features = [f'WeightedAvg_Property{i}' for i in range(1, 11)]
target_columns = [f'BlendProperty{i}' for i in range(1, 11)]

X = train[weighted_features]
y = train[target_columns]

# Split into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Train DummyRegressor
dummy = MultiOutputRegressor(DummyRegressor(strategy='mean'))
dummy.fit(X_train, y_train)
y_dummy_pred = dummy.predict(X_val)
dummy_mape = mean_absolute_percentage_error(y_val, y_dummy_pred)
print(f"üìâ Dummy Regressor MAPE: {dummy_mape:.4f}")

# Train LinearRegression
linreg = MultiOutputRegressor(LinearRegression())
linreg.fit(X_train, y_train)
y_linreg_pred = linreg.predict(X_val)
linreg_mape = mean_absolute_percentage_error(y_val, y_linreg_pred)
print(f"üìà Linear Regression MAPE: {linreg_mape:.4f}")

"""step 4 very easy and simple model moving onto xgboost modelling for handling non linear relationships"""

!pip install xgboost

import xgboost as xgb
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_percentage_error

# Define input and target variables
X = train[[f'WeightedAvg_Property{i}' for i in range(1, 11)]]
y = train[[f'BlendProperty{i}' for i in range(1, 11)]]

# Split into training and validation
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the XGBoost model
xgb_model = MultiOutputRegressor(
    xgb.XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, objective='reg:squarederror', random_state=42)
)

xgb_model.fit(X_train, y_train)

# Predict and evaluate MAPE
y_xgb_pred = xgb_model.predict(X_val)
xgb_mape = mean_absolute_percentage_error(y_val, y_xgb_pred)

print(f"üî• XGBoost MAPE: {xgb_mape:.4f}")

"""using xgboost it is worse than tghe baseline model moving onto tuning it"""

# Step 1: Use all informative features
component_features = [col for col in train.columns if 'Component' in col and 'Property' in col]
fraction_features = [f'Component{i}_fraction' for i in range(1, 6)]
weighted_features = [f'WeightedAvg_Property{i}' for i in range(1, 11)]

# Final feature set
all_features = component_features + fraction_features + weighted_features

X = train[all_features]
y = train[[f'BlendProperty{i}' for i in range(1, 11)]]

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

import xgboost as xgb
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_percentage_error

# More powerful XGBoost
xgb_model = MultiOutputRegressor(
    xgb.XGBRegressor(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        objective='reg:squarederror',
        random_state=42
    )
)

xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_val)

# MAPE evaluation
mape = mean_absolute_percentage_error(y_val, y_pred)
print(f"üîÅ Improved XGBoost MAPE: {mape:.4f}")

"""model is good improving further"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer
import xgboost as xgb
import numpy as np

# Use full feature set you defined earlier
X = train[all_features]
y = train['BlendProperty1']  # Tune only on one target

# Split into training and validation
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Define MAPE scorer
mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)

# Define hyperparameter space
param_dist = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [3, 4, 5, 6, 7],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

# Run RandomizedSearchCV
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
search = RandomizedSearchCV(
    xgb_model,
    param_distributions=param_dist,
    n_iter=25,
    scoring=mape_scorer,
    cv=3,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

search.fit(X_train, y_train)

# Best parameters and score
print("‚úÖ Best Parameters Found:")
print(search.best_params_)
print(f"üîç Best MAPE (negative): {search.best_score_:.4f}")

from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.model_selection import train_test_split
import xgboost as xgb

# Use the same feature set as earlier
X = train[all_features]
y = train[[f'BlendProperty{i}' for i in range(1, 11)]]

# Split into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Best parameters from tuning
best_params = {
    'subsample': 0.8,
    'n_estimators': 300,
    'max_depth': 3,
    'learning_rate': 0.1,
    'colsample_bytree': 0.8,
    'objective': 'reg:squarederror',
    'random_state': 42
}

# Train MultiOutput XGBoost using the best parameters
xgb_model = MultiOutputRegressor(xgb.XGBRegressor(**best_params))
xgb_model.fit(X_train, y_train)

# Predict and evaluate
y_pred = xgb_model.predict(X_val)
final_mape = mean_absolute_percentage_error(y_val, y_pred)

print(f"‚úÖ Final XGBoost MAPE (All Targets): {final_mape:.4f}")

"""step 7 finished"""

# Create 50 interaction features
for i in range(1, 6):  # Component 1 to 5
    for j in range(1, 11):  # Property 1 to 10
        frac_col = f'Component{i}_fraction'
        prop_col = f'Component{i}_Property{j}'
        new_col = f'Interaction_C{i}_P{j}'

        train[new_col] = train[frac_col] * train[prop_col]
        test[new_col] = test[frac_col] * test[prop_col]

# Add interaction features to your feature list
interaction_features = [f'Interaction_C{i}_P{j}' for i in range(1, 6) for j in range(1, 11)]
enhanced_features = all_features + interaction_features  # full feature set

from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error
import xgboost as xgb

# Use enhanced feature set
X = train[enhanced_features]
y = train[[f'BlendProperty{i}' for i in range(1, 11)]]

# Train/validation split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# XGBoost default config (which gave best earlier)
xgb_model = MultiOutputRegressor(
    xgb.XGBRegressor(
        n_estimators=300,
        max_depth=4,
        learning_rate=0.1,
        objective='reg:squarederror',
        random_state=42
    )
)

# Train and evaluate
xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_val)
mape_enhanced = mean_absolute_percentage_error(y_val, y_pred)

print(f"üî• Enhanced XGBoost MAPE (with interaction features): {mape_enhanced:.4f}")

pd.DataFrame(test_preds).describe()

print("Features used for prediction:", enhanced_features)
print("Does test contain these columns?", all([f in test.columns for f in enhanced_features]))

import numpy as np
import pandas as pd

# Reset indices to ensure we can sample correctly
X_val_reset = X_val.reset_index(drop=True)
y_val_reset = y_val.reset_index(drop=True)

# Predict again (if needed) and wrap into DataFrame
y_pred_df = pd.DataFrame(y_pred, columns=[f'BlendProperty{i}' for i in range(1, 11)])

# Choose N random samples from validation set
N = 10
sample_idx = np.random.choice(len(y_val_reset), size=N, replace=False)

# Create a comparison DataFrame
comparison = pd.DataFrame()

for i in range(10):
    actual_col = f'Actual_BlendProperty{i+1}'
    pred_col = f'Predicted_BlendProperty{i+1}'
    error_col = f'Error_BlendProperty{i+1} (%)'

    comparison[actual_col] = y_val_reset.iloc[sample_idx, i].values
    comparison[pred_col] = y_pred_df.iloc[sample_idx, i].values
    comparison[error_col] = 100 * np.abs(comparison[actual_col] - comparison[pred_col]) / np.abs(comparison[actual_col] + 1e-6)  # add small value to avoid division by zero

# Show comparison
pd.set_option("display.precision", 4)
comparison

import joblib

# Save your trained model (xgb_model, lgb_model, etc.)
joblib.dump(xgb_model, "final_model.pkl")
print("‚úÖ Model saved to final_model.pkl")

# Load the model back (for submission or reuse)
model = joblib.load("final_model.pkl")

# Predict on test set using same features used during training
test_preds = model.predict(test[enhanced_features])

# Wrap predictions in a DataFrame
solution = pd.DataFrame(test_preds, columns=[f'BlendProperty{i}' for i in range(1, 11)])

# Insert ID column: 1 to 500
solution.insert(0, 'ID', range(1, 501))

# Save the solution file
solution.to_csv("solution.csv", index=False)

# Download the file from Colab
from google.colab import files
files.download("solution.csv")
print("üì¶ solution.csv ready for Shell.ai submission ‚úÖ")

"""the model is devloped before this step and the prediction file is downloaded this is the final step of devloping the file"""

# Example fix: add an ID column 1‚Ä¶500
import pandas as pd
sample = pd.read_csv('sample_solution.csv')
print("Sample shape:", sample.shape)
print("Sample columns:", sample.columns.tolist())
print(sample.head())

sol = pd.read_csv('solution.csv', header=None)
sol.insert(0, 'ID', sample['ID'])
sol.to_csv('solution.csv', index=False)

from google.colab import files
files.download('solution.csv')

"""until this step run all the code before it

Your Current Situation (Summarized)
Item	Status
üîß Model Accuracy	MAPE = ~1.08% ‚Äì very strong, real-world usable
üìä Output Analysis	Accurate in most cases; minor outliers are explainable
üß† Validation Check	Model understands fuel behavior, passed proof test
‚è∞ Time Left	~14 days
üèÜ Goal	Win or rank high in Shell.ai Hackathon

üß† What Does This Mean for Your Chances?
Your model is:

Well-designed

Built on meaningful domain logic (fractions, weighted features, interactions)

Shows solid performance, better than a baseline

Flexible enough to improve further

üí° So yes ‚Äî this model can absolutely help you win or rank high, especially if most teams only do basic modeling.

üî• My Recommendation
With 14 days left, here‚Äôs a smart strategy:

‚úÖ 1. Save your current model as your fallback
Store the code + predictions

You can submit this any time with confidence

üîÅ 2. Spend the next 5‚Äì7 days improving in 1 or 2 areas
üéØ Try one or both:
Step 9: Ensembling

Combine your current XGBoost with RandomForest or LightGBM

Could lower MAPE slightly (from 1.08 ‚Üí 1.02 or so)

Fine-tune outlier handling

Use MAE + MAPE together

Apply log-scaling on small target values if needed

üß† 3. Spend the final 4‚Äì5 days polishing:
Code structure (functions, comments)

Add visualizations (feature importance, error distributions)

Prepare 1-page model summary for presentation/report if needed

‚úÖ 4. Submit with confidence
If leaderboard submission is allowed:

Submit your best prediction file

Optionally submit both raw XGBoost and ensembled version to see which ranks better

üí° TL;DR: Here‚Äôs Your Smart 14-Day Plan
Days	Task
1‚Äì2	Freeze & back up current MVP
3‚Äì6	Try ensembling + MAE/MAPE tuning
7‚Äì10	Validate with test set predictions
11‚Äì14	Prepare final code & documentation

Would you like to proceed with:

üîÄ Step 9: Ensembling (small boost)?

üì¶ Step 10: Test predictions & submission formatting (get it done)?

Let me know ‚Äî I‚Äôll guide you step by step.









Ask ChatGPT
"""

from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_percentage_error
import lightgbm as lgb
import numpy as np
import pandas as pd

# ‚öôÔ∏è Optional: filter down to enhanced_features (already in your code)
# features = enhanced_features

# Step 1: Log-transform the targets
y_train_log = np.log1p(y_train)
y_val_log = np.log1p(y_val)

# Step 2: Define LightGBM model with tuned parameters
lgb_params = {
    'objective': 'regression',
    'learning_rate': 0.05,
    'n_estimators': 1000,
    'max_depth': 6,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'reg_alpha': 0.1,
    'reg_lambda': 0.3,
    'random_state': 42
}

lgb_model = MultiOutputRegressor(
    lgb.LGBMRegressor(**lgb_params)
)

# Step 3: Fit the model
lgb_model.fit(
    X_train[enhanced_features], y_train_log,
    # LightGBM handles early stopping only on single-output; this will skip it in MultiOutputRegressor
)

# Step 4: Predict on validation and evaluate MAPE
y_val_pred_log = lgb_model.predict(X_val[enhanced_features])
y_val_pred = np.expm1(y_val_pred_log)
y_val_orig = np.expm1(y_val_log)

lgb_mape = mean_absolute_percentage_error(y_val_orig, y_val_pred)
print(f"üìâ LightGBM Validation MAPE (log-transformed): {lgb_mape:.4f}")

"""the model is giving MAPE of 1.08  further decreasing the MAPE

implementing catboost got 1.0053 best so far
"""

# 1. Install & import
!pip install catboost

from catboost import CatBoostRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.model_selection import train_test_split

# 2. Prepare data
X = train[enhanced_features]
y = train[[f'BlendProperty{i}' for i in range(1, 11)]]

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3. Build CatBoost multi-output regressor
cat_model = MultiOutputRegressor(
    CatBoostRegressor(
        iterations=500,
        learning_rate=0.05,
        depth=6,
        loss_function='MAPE',
        verbose=0,
        random_seed=42
    )
)

# 4. Train
cat_model.fit(X_train, y_train)

# 5. Validate
y_cat_pred = cat_model.predict(X_val)
cat_mape = mean_absolute_percentage_error(y_val, y_cat_pred)
print(f"ü§ñ CatBoost MultiOutput MAPE: {cat_mape:.4f}")

# 6. If it beats your XGB (1.08%), generate submission:

if cat_mape < 1.08:
    # Predict on test set
    test_cat_preds = cat_model.predict(test[enhanced_features])
    # Build solution DataFrame
    solution = pd.DataFrame(test_cat_preds, columns=[f'BlendProperty{i}' for i in range(1, 11)])
    solution.insert(0, 'ID', range(1, len(solution)+1))
    solution.to_csv("solution_catboost.csv", index=False)
    print("üì¶ solution_catboost.csv saved‚Äîdownload & submit it for evaluation!")
else:
    print("‚ö†Ô∏è CatBoost did not improve; consider ensemble or further tuning.")

import pandas as pd
from google.colab import files

# 1. Predict on the test set with your trained CatBoost model
test_cat_preds = cat_model.predict(test[enhanced_features])

# 2. Build the submission DataFrame
solution_cat = pd.DataFrame(
    test_cat_preds,
    columns=[f'BlendProperty{i}' for i in range(1, 11)]
)
solution_cat.insert(0, 'ID', range(1, len(solution_cat) + 1))

# 3. Save to CSV
solution_cat.to_csv("solution_catboost.csv", index=False)

# 4. Download for upload
files.download("solution_catboost.csv")

"""TRAINING LIGHTGBM"""

# 1) Install & import
!pip install lightgbm

import numpy as np
import pandas as pd
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error

# 2) Prepare features & targets
X = train[enhanced_features]
y = train[[f'BlendProperty{i}' for i in range(1, 11)]]

# 3) Train/validation split
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4) Build & train LightGBM multi-output regressor
lgb_model = MultiOutputRegressor(
    LGBMRegressor(
        objective='regression',
        learning_rate=0.05,
        n_estimators=500,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,
        reg_lambda=0.3,
        random_state=42
    )
)

lgb_model.fit(X_train, y_train)

# 5) Validate: compute MAPE on hold‚Äëout
y_val_pred = lgb_model.predict(X_val)
lgb_mape = mean_absolute_percentage_error(y_val, y_val_pred)
print(f"üìâ LightGBM Validation MAPE: {lgb_mape:.4f}")

# 6) Predict on test set
test_preds = lgb_model.predict(test[enhanced_features])

# 7) Build submission DataFrame
solution_lgb = pd.DataFrame(
    test_preds,
    columns=[f'BlendProperty{i}' for i in range(1, 11)]
)
solution_lgb.insert(0, 'ID', range(1, len(solution_lgb) + 1))

# 8) Save & download
solution_lgb.to_csv("solution_lgb.csv", index=False)
from google.colab import files
files.download("solution_lgb.csv")

print("‚úÖ solution_lgb.csv saved ‚Äî upload it to Shell.ai to check your score!")

import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.multioutput import MultiOutputRegressor
from sklearn.base import clone

# 1) Define base models dict
models = {
    "xgb": xgb_model,
    "cat": cat_model,
    "lgb": lgb_model
}

# 2) Prepare OOF & test containers
n_splits = 5
kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
oof_preds = np.zeros((len(train), 10))
test_preds = np.zeros((len(test), 10, len(models)))

# 3) Generate OOF & test preds
for fold, (tr_idx, val_idx) in enumerate(kf.split(train)):
    print(f"\nüîÅ Fold {fold+1}/{n_splits}")
    X_tr = train.iloc[tr_idx][enhanced_features]
    X_val = train.iloc[val_idx][enhanced_features]
    y_tr = train.iloc[tr_idx][target_cols]

    for m_idx, (name, model) in enumerate(models.items()):
        print(f"  ‚Ä¢ {name}")
        # Clone base estimator
        base = model.estimator if hasattr(model, "estimator") else model
        wrapper = MultiOutputRegressor(clone(base))
        wrapper.fit(X_tr, y_tr)

        # OOF prediction
        oof_preds[val_idx] += wrapper.predict(X_val) / len(models)
        # Test prediction
        test_preds[:, :, m_idx] += wrapper.predict(test[enhanced_features]) / n_splits

from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor

# 4‚Ä≤) Train LightGBM meta‚Äëmodel on OOF
meta_lgb = MultiOutputRegressor(
    LGBMRegressor(
        n_estimators=200,
        learning_rate=0.05,
        max_depth=3,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42
    )
)
meta_lgb.fit(oof_preds, train[target_cols])

# 5‚Ä≤) Final blended predictions
final_preds_lgb = meta_lgb.predict(avg_test)

# 6‚Ä≤) Evaluate OOF MAPE
oof_mape_lgb = mean_absolute_percentage_error(train[target_cols], meta_lgb.predict(oof_preds))
print(f"üîÄ Stacked + LGB Meta OOF MAPE: {oof_mape_lgb:.4f}")

# 7‚Ä≤) Save CSV
sol = pd.DataFrame(final_preds_lgb, columns=target_cols)
sol.insert(0, 'ID', range(1, len(sol)+1))
sol.to_csv("solution_stacked_lgbmeta.csv", index=False)
from google.colab import files
files.download("solution_stacked_lgbmeta.csv")

"""used ensembling stacking technqieu of xgboost,catboost and lightgbm and got a score of 84 in shell best so far

going to do log based methods to improve score
"""

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error
from google.colab import files

# Assume final_preds (500√ó10) and oof_preds (2000√ó10) exist
special_targets = [1, 8]
improved_test_preds = final_preds.copy()
improved_oof_preds = oof_preds.copy()

for t in special_targets:
    col = f"BlendProperty{t}"
    print(f"\nüîß Training log‚Äëspecialist for {col}")

    # 1) Filter valid rows (y > -1)
    y = train[col]
    valid = y > -1
    X_valid = train.loc[valid, enhanced_features]
    y_valid = y[valid]

    # 2) Log‚Äëtransform
    y_log = np.log1p(y_valid)

    # 3) Train‚Äëvalidation split (optional for sanity check)
    X_tr, X_va, y_tr_log, y_va_log = train_test_split(
        X_valid, y_log, test_size=0.2, random_state=42
    )

    # 4) Train full XGBoost
    log_model = xgb.XGBRegressor(
        n_estimators=400,
        max_depth=4,
        learning_rate=0.05,
        objective="reg:squarederror",
        random_state=42,
        use_label_encoder=False,
        eval_metric="mae"
    )
    log_model.fit(X_tr, y_tr_log, verbose=False)

    # 5) Sanity check on validation
    y_va_pred_log = log_model.predict(X_va)
    y_va_pred = np.expm1(y_va_pred_log)
    va_mape = mean_absolute_percentage_error(np.expm1(y_va_log), y_va_pred)
    print(f"   ‚ñ∂Ô∏è {col} hold‚Äëout MAPE: {va_mape:.4f}")

    # 6) Override stacking preds
    train_log_pred = log_model.predict(train[enhanced_features])
    improved_oof_preds[:, t - 1] = np.expm1(train_log_pred)

    test_log_pred = log_model.predict(test[enhanced_features])
    improved_test_preds[:, t - 1] = np.expm1(test_log_pred)

# 7) Save new solution
solution = pd.DataFrame(
    improved_test_preds,
    columns=[f"BlendProperty{i}" for i in range(1, 11)]
)
solution.insert(0, "ID", range(1, len(solution) + 1))
solution.to_csv("solution_log_specialists.csv", index=False)
files.download("solution_log_specialists.csv")
print("\nüöÄ solution_log_specialists.csv ready‚Äîupload to Shell.ai!")

"""training MLP and NGboost"""

!pip install ngboost

# === Imports ===
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error

# PyTorch for MLP
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# NGBoost wrapped for multi‚Äëoutput
from ngboost import NGBRegressor
from ngboost.distns import Normal
from sklearn.multioutput import MultiOutputRegressor

# === Prepare Data ===
X = train[enhanced_features].values
y = train[target_cols].values
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# === A) MLP Model ===
class BlendNet(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, 128), nn.ReLU(), nn.BatchNorm1d(128), nn.Dropout(0.2),
            nn.Linear(128,  64), nn.ReLU(), nn.BatchNorm1d(64),  nn.Dropout(0.2),
            nn.Linear(64,   32), nn.ReLU(), nn.BatchNorm1d(32),
            nn.Linear(32, out_dim)
        )
    def forward(self, x):
        return self.net(x)

# DataLoader
train_ds = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)

# Instantiate
mlp = BlendNet(X_train.shape[1], y_train.shape[1])
optimizer = optim.Adam(mlp.parameters(), lr=1e-3)
loss_fn = nn.L1Loss()

# Train MLP
mlp.train()
for epoch in range(20):
    for xb, yb in train_loader:
        optimizer.zero_grad()
        preds = mlp(xb)
        loss_fn(preds, yb).backward()
        optimizer.step()

# Validate MLP
mlp.eval()
with torch.no_grad():
    val_preds = mlp(torch.FloatTensor(X_val)).numpy()
mlp_mape = mean_absolute_percentage_error(y_val, val_preds)
print(f"üî• MLP Validation MAPE: {mlp_mape:.4f}")

# Save MLP submission
mlp_test_preds = mlp(torch.FloatTensor(test[enhanced_features].values)).detach().numpy()
solution_mlp = pd.DataFrame(mlp_test_preds, columns=target_cols)
solution_mlp.insert(0, 'ID', range(1, len(solution_mlp) + 1))
solution_mlp.to_csv("solution_mlp.csv", index=False)
print("‚úÖ solution_mlp.csv saved")

# === B) NGBoost Multi‚ÄëOutput Model ===
ngb_base = NGBRegressor(
    Dist=Normal,
    n_estimators=500,
    learning_rate=0.05,
    verbose=False
)
ngb_model = MultiOutputRegressor(ngb_base)
ngb_model.fit(X_train, y_train)

# Validate NGBoost
ngb_val_preds = ngb_model.predict(X_val)
ngb_mape = mean_absolute_percentage_error(y_val, ngb_val_preds)
print(f"ü§ñ NGBoost Validation MAPE: {ngb_mape:.4f}")

# Save NGBoost submission
ngb_test_preds = ngb_model.predict(test[enhanced_features].values)
solution_ngb = pd.DataFrame(ngb_test_preds, columns=target_cols)
solution_ngb.insert(0, 'ID', range(1, len(solution_ngb) + 1))
solution_ngb.to_csv("solution_ngb.csv", index=False)
print("‚úÖ solution_ngb.csv saved")